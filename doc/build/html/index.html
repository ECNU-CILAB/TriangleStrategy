<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Welcome to TriangleStrategy’s documentation! &mdash; TriangleStrategy 1.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="TriangleStrategy package" href="TriangleStrategy.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> TriangleStrategy
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="TriangleStrategy.html">TriangleStrategy package</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">TriangleStrategy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>Welcome to TriangleStrategy’s documentation!</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="welcome-to-trianglestrategy-s-documentation">
<h1>Welcome to TriangleStrategy’s documentation!<a class="headerlink" href="#welcome-to-trianglestrategy-s-documentation" title="Permalink to this headline"></a></h1>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">API Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="TriangleStrategy.html">TriangleStrategy package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="TriangleStrategy.Methods.html">TriangleStrategy.Methods package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="TriangleStrategy.Methods.html#module-TriangleStrategy.Methods.A2C">TriangleStrategy.Methods.A2C module</a></li>
<li class="toctree-l3"><a class="reference internal" href="TriangleStrategy.Methods.html#module-TriangleStrategy.Methods.DDPG">TriangleStrategy.Methods.DDPG module</a></li>
<li class="toctree-l3"><a class="reference internal" href="TriangleStrategy.Methods.html#module-TriangleStrategy.Methods.DQN">TriangleStrategy.Methods.DQN module</a></li>
<li class="toctree-l3"><a class="reference internal" href="TriangleStrategy.Methods.html#module-TriangleStrategy.Methods.DQNA">TriangleStrategy.Methods.DQNA module</a></li>
<li class="toctree-l3"><a class="reference internal" href="TriangleStrategy.Methods.html#module-TriangleStrategy.Methods.OASS">TriangleStrategy.Methods.OASS module</a></li>
<li class="toctree-l3"><a class="reference internal" href="TriangleStrategy.Methods.html#module-TriangleStrategy.Methods.PG">TriangleStrategy.Methods.PG module</a></li>
<li class="toctree-l3"><a class="reference internal" href="TriangleStrategy.Methods.html#module-TriangleStrategy.Methods.PPO">TriangleStrategy.Methods.PPO module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="TriangleStrategy.html#module-TriangleStrategy.Environment">TriangleStrategy.Environment module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TriangleStrategy.html#module-TriangleStrategy.ModelArchitecture">TriangleStrategy.ModelArchitecture module</a></li>
<li class="toctree-l2"><a class="reference internal" href="TriangleStrategy.html#module-TriangleStrategy.TrainFunctions">TriangleStrategy.TrainFunctions module</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="project-description">
<h1>Project Description<a class="headerlink" href="#project-description" title="Permalink to this headline"></a></h1>
<p>TriangleStrategy is a high-efficiency reinforcement learning based algorithmic trading library. We provide examples of mainstream general-purpose reinforcement learning algorithms, as well as several reinforcement learning algorithms designed specifically for algorithmic trading. We currently support the following methods:</p>
<ul>
<li><p>General reinforcement learning methods</p>
<blockquote>
<div><p><a class="reference external" href="https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf">PG</a></p>
<p><a class="reference external" href="https://openai.com/blog/baselines-acktr-a2c/">A2C</a></p>
<p><a class="reference external" href="https://www.nature.com/articles/nature14236">DQN</a></p>
<p><a class="reference external" href="https://arxiv.org/pdf/1509.02971.pdf">DDPG</a></p>
<p><a class="reference external" href="https://arxiv.org/pdf/1707.06347.pdf">PPO</a></p>
</div></blockquote>
</li>
<li><p>Specifical reinforcement learning methods designed for algorithmic trading</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/pdf/1807.02787">DQN-A</a></p>
<p>OASS (Our proposed method, coming sooooooooooon…)</p>
</div></blockquote>
</li>
</ul>
<p>This project is open source and is jointly supported by <strong>East China Normal University</strong> and <strong>Seek Data Group, Emoney Inc.</strong>.</p>
<section id="project-architecture">
<h2>1. Project Architecture<a class="headerlink" href="#project-architecture" title="Permalink to this headline"></a></h2>
<p>TriangleStrategy implements algorithmic trading strategies based on reinforcement learning primarily through the following modules:</p>
<ul class="simple">
<li><p>Environment</p></li>
<li><p>ModelArchitecture</p></li>
<li><p>Methods (DecisionMaker, Trainer)</p></li>
<li><p>TrainFunctions</p></li>
</ul>
<section id="environment-reward-calculater">
<h3>1.1 Environment: reward calculater<a class="headerlink" href="#environment-reward-calculater" title="Permalink to this headline"></a></h3>
<p>Currently, we only provide a generic environment that is used to receive actions and calculate rewards to guide model training.</p>
<p>The environment supports two action spaces:</p>
<ul class="simple">
<li><p>Discrete action space {-1, 0, 1}</p></li>
<li><p>Continuous action space [-1, 1]</p></li>
</ul>
<p>The action value is the signal to adjust the position. For example, when action 1 is selected, the agent will hold 1 unit of the financial product but not buy 1 unit of the financial product. This design prevents the agent from overbuying a financial product, which can lead to significant risk. Negative action values are allowed in both the discrete and continuous action spaces. When action -1 is selected, the agent will adjust to a state of holding -1 units of the financial product. In other words, the agent sells 1 unit of the financial product in the underlying position and earns a gain from the fall in price if it can buy it back in the future at a lower price. The negative part of the action space allows the model to better perceive price declines.</p>
<p>The calculation of the reward value provides both the amount of change in assets and the amount of change in money. The difference is that the latter does not include the value of the financial product and is negative when buying and positive when selling. If the amount of change in money is used, then obviously agent only needs to sell to get a high reward. to avoid this, when using the discrete action space, the first and last action in the sequence must be 0. For the continuous action space, we do not make a similar restriction for now, but we still recommend to implement it manually when using it.</p>
</section>
<section id="modelarchitecture-basic-neural-network-models">
<h3>1.2 ModelArchitecture: basic neural network models<a class="headerlink" href="#modelarchitecture-basic-neural-network-models" title="Permalink to this headline"></a></h3>
<p>Model structures in reinforcement learning algorithms are independent of the algorithm, so theoretically any neural network model structure can be used as long as the input and output layers are in corresponding formats, and we provide several basic model structures. The main part of these model structures are all multilayer LSTMs, which are identical except for the input and output layers.</p>
</section>
<section id="methods-reinforcement-learning-methods">
<h3>1.3 Methods: reinforcement learning methods<a class="headerlink" href="#methods-reinforcement-learning-methods" title="Permalink to this headline"></a></h3>
<p>Each method is implemented using <code class="docutils literal notranslate"><span class="pre">DecisionMaker</span></code> and <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">DecisionMaker</span></code> is used to generate action values based on the output values of the model. There are two parameters in each <code class="docutils literal notranslate"><span class="pre">DecisionMaker</span></code>: <code class="docutils literal notranslate"><span class="pre">head_mask</span></code>, <code class="docutils literal notranslate"><span class="pre">tail_mask</span></code>. Considering the structure of the LSTM model, there is not enough historical information for the model to make stable decisions in the first few time steps of the sequence, so <code class="docutils literal notranslate"><span class="pre">DecisionMaker</span></code> forces the action value of <code class="docutils literal notranslate"><span class="pre">i&lt;head_mask</span></code> to 0. On the other hand, in the last few time steps of the sequence, there is not enough future information for the environment to On the other hand, in the last few time steps of the sequence, there is not enough future information for the environment to generate a reward, so <code class="docutils literal notranslate"><span class="pre">DecisionMaker</span></code> forces the action value of <code class="docutils literal notranslate"><span class="pre">i&gt;=tail_mask</span></code> to 0. In addition, <code class="docutils literal notranslate"><span class="pre">DecisionMaker</span></code> can be used both for training and evaluation, and can be switched by controlling the <code class="docutils literal notranslate"><span class="pre">training</span></code> parameter in it.</p>
<p><code class="docutils literal notranslate"><span class="pre">Trainer</span></code> will call <code class="docutils literal notranslate"><span class="pre">model</span></code>, <code class="docutils literal notranslate"><span class="pre">DecisionMaker</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer</span></code>, and <code class="docutils literal notranslate"><span class="pre">data_loader</span></code> for training. If you want to design a new reinforcement learning method, this part will be the core part. The design of <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> is very free, and the few methods of <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> in this library do not even need to depend on Environment. This can significantly improve the training efficiency.</p>
</section>
<section id="trainfunctions-functions-used-to-assist-in-training">
<h3>1.4 TrainFunctions: Functions used to assist in training<a class="headerlink" href="#trainfunctions-functions-used-to-assist-in-training" title="Permalink to this headline"></a></h3>
<p>In our experiments, if trained directly, many methods would guide the model to produce 0 decisions, since doing so would clearly hedge the risks in financial markets. To avoid this, we borrow the idea of curriculum learning. <code class="docutils literal notranslate"><span class="pre">TrainFunctions</span></code> provides functions for curriculum learning, which controls the <code class="docutils literal notranslate"><span class="pre">difficulty</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>, and thus the transaction cost when the model trades. The model is first trained in low transaction cost, and then the transaction cost is gradually increased to reach the real transaction cost.</p>
</section>
</section>
<section id="code-examples">
<h2>2. Code examples<a class="headerlink" href="#code-examples" title="Permalink to this headline"></a></h2>
<p>Take DQN as an example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">TriangleStrategy</span> <span class="k">as</span> <span class="nn">ts</span>


<span class="c1"># device</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># data loader</span>
<span class="n">data_loader_train</span> <span class="o">=</span> <span class="n">ts</span><span class="o">.</span><span class="n">get_DataLoader</span><span class="p">(</span>
    <span class="s2">&quot;normalized_train_data.npz&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span>
<span class="p">)</span>
<span class="n">data_loader_dev</span> <span class="o">=</span> <span class="n">ts</span><span class="o">.</span><span class="n">get_DataLoader</span><span class="p">(</span>
    <span class="s2">&quot;normalized_dev_data.npz&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span>
<span class="p">)</span>
<span class="n">data_loader_test</span> <span class="o">=</span> <span class="n">ts</span><span class="o">.</span><span class="n">get_DataLoader</span><span class="p">(</span>
    <span class="s2">&quot;normalized_test_data.npz&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span>
<span class="p">)</span>
<span class="c1"># model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ts</span><span class="o">.</span><span class="n">ModelGinga</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_target</span> <span class="o">=</span> <span class="n">ts</span><span class="o">.</span><span class="n">ModelGinga</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_target</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
<span class="c1"># optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="c1"># trainer (Note that the environment is included in the trainer.)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">ts</span><span class="o">.</span><span class="n">TrainerDQN</span><span class="p">(</span>
    <span class="n">head_mask</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">tail_mask</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
    <span class="n">buy_cost_pct</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">sell_cost_pct</span><span class="o">=</span><span class="mf">0.001</span>
<span class="p">)</span>
<span class="c1"># At the begining of training process, use a decision maker that chooses actions randomly.</span>
<span class="n">ts</span><span class="o">.</span><span class="n">train_model_until_converge</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">data_loader_train</span><span class="p">,</span> <span class="n">data_loader_dev</span><span class="p">,</span> <span class="n">data_loader_test</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span>
    <span class="n">decision_maker</span><span class="o">=</span><span class="n">ts</span><span class="o">.</span><span class="n">DecisionMakerDQN</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">head_mask</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tail_mask</span><span class="o">=</span><span class="mi">120</span><span class="p">),</span>
    <span class="n">model_target</span><span class="o">=</span><span class="n">model_target</span><span class="p">,</span>
    <span class="n">epoch_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">epoch_window</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">epoch_window_win</span><span class="o">=</span><span class="mi">16</span>
<span class="p">)</span>
<span class="c1"># Then increase the difficulty linearly.</span>
<span class="n">ts</span><span class="o">.</span><span class="n">heuristic_train_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">data_loader_train</span><span class="p">,</span> <span class="n">data_loader_dev</span><span class="p">,</span> <span class="n">data_loader_test</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span>
    <span class="n">decision_maker</span><span class="o">=</span><span class="n">ts</span><span class="o">.</span><span class="n">DecisionMakerDQN</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">head_mask</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tail_mask</span><span class="o">=</span><span class="mi">120</span><span class="p">),</span>
    <span class="n">model_target</span><span class="o">=</span><span class="n">model_target</span><span class="p">,</span>
    <span class="n">steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">epoch_max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">epoch_window</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">epoch_window_win</span><span class="o">=</span><span class="mi">16</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="datasets">
<h2>3. Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline"></a></h2>
<p>The dataset is stored in <code class="docutils literal notranslate"><span class="pre">.npz</span></code> format. If you want to build the dataset yourself using another data source, use the <code class="docutils literal notranslate"><span class="pre">numpy.savez_compressed</span></code> function to get the <code class="docutils literal notranslate"><span class="pre">.npz</span></code> file. The data file nust contain the following three parts:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_data</span></code>: Environment states at each time step. This part is the representation of the market condition and is inputed to the model. The shape is <code class="docutils literal notranslate"><span class="pre">(sequence_num,</span> <span class="pre">sequence_length,</span> <span class="pre">input_size)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buy_price</span></code>: The transaction price when the agent chooses to buy at the corresponding time step. The shape is <code class="docutils literal notranslate"><span class="pre">(sequence_num,</span> <span class="pre">sequence_length)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sell_price</span></code>: The transaction price when the agent chooses to sell at the corresponding time step. The shape is <code class="docutils literal notranslate"><span class="pre">(sequence_num,</span> <span class="pre">sequence_length)</span></code>.</p></li>
</ul>
<p>Usually the <code class="docutils literal notranslate"><span class="pre">buy_price</span></code> is higher than the <code class="docutils literal notranslate"><span class="pre">sell_price</span></code>, but you can use the same values.</p>
<p>In addition, we provide two datasets collected from China stock market.</p>
<ul class="simple">
<li><p><strong>TriangleStrategy-minute</strong> is high-frequency limited order book (LOB) data, including the minute-level price and volume information. The ask price and bid price are used as the transaction price. There are 240 time steps in each sequence, which corresponds to 240 minutes in a trading day.</p></li>
<li><p><strong>TriangleStrategy-day</strong> is low-frequency data, including day-level market information. The close price every day is used as the transaction price. Considering that the data amount of low-frequency trading is much smaller than that of high-frequency trading, we collect low-frequency trading data over a larger time span. There are 140 time steps in each sequence, which corresponds to 140 continuous trading days.</p></li>
</ul>
<p>To gain access to the dataset, please refer to <a class="reference external" href="http://www.seek-data.com/research.html">http://www.seek-data.com/research.html</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="TriangleStrategy.html" class="btn btn-neutral float-right" title="TriangleStrategy package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Artiprocher.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>